{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Summarization\n",
        "Nama: Raihan Satriya Bagaskara\n",
        "\n",
        "NIM: A11.2020.12757"
      ],
      "metadata": {
        "id": "5XOJY5mxbGx2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DMUAdy3YZNhY"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import networkx as nx\n",
        "from nltk.tokenize import  sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('articles.csv', sep=',')"
      ],
      "metadata": {
        "id": "DNRcMVVbbbZN"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X44OKvVl99d",
        "outputId": "fb34ab11-c488-4eae-fce1-1eaff3f61dd1"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Title', 'Link', 'Text', 'ImgUrl']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articles = df['Text'].tolist()"
      ],
      "metadata": {
        "id": "2OPTbp4ebn-P"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "  return 1 - cosine_distance(vector1, vector2)"
      ],
      "metadata": {
        "id": "lVX9VS0qb51b"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_similarity_matrix(sentences, stop_words):\n",
        "  return similarity_matrix"
      ],
      "metadata": {
        "id": "pesOLTpecLZ-"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_sentences = []\n",
        "for article in articles:\n",
        "    # Convert the article to string\n",
        "    article = str (article).lower ()\n",
        "    sentences = sent_tokenize (article)\n",
        "    for sentence in sentences:\n",
        "        clean_sentence = re.sub (\"[^a-zA-Z]\", \" \", sentence.lower ())\n",
        "        cleaned_sentences.append (clean_sentence)"
      ],
      "metadata": {
        "id": "y_xsuna8hn2v"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_similarity_matrix(sentences, stop_words):\n",
        "    # Create an empty numpy array with the shape of (len(sentences), len(sentences))\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "    # Loop through each pair of sentences and calculate their similarity\n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            # Skip the diagonal elements\n",
        "            if idx1 == idx2:\n",
        "                continue\n",
        "            # Call the sentence_similarity function and assign the result to the matrix element\n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "    # Return the similarity matrix\n",
        "    return similarity_matrix\n"
      ],
      "metadata": {
        "id": "f3Sg9QPyb66K"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ranked_sentence = sorted(((v, cleaned_sentences[k]) for k, v in scores.items()), reverse=True)"
      ],
      "metadata": {
        "id": "AhiGI95vcQRp"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ranked_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URJOO7t0iiOM",
        "outputId": "a8496276-ab8f-4dd8-b60b-65b7b7f20dcf"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Summary:\")\n",
        "print(ranked_sentence[3][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw-cHvg-jKw7",
        "outputId": "569c7dba-6927-4cad-8ef5-03b97654bf20"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "i m currently populating it with some of the better comics and will resume posting new content soon \n"
          ]
        }
      ]
    }
  ]
}